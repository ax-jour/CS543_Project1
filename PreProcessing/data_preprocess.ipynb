{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create or get a PySpark instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 21:17:05 WARN Utils: Your hostname, LinuxGUI resolves to a loopback address: 127.0.1.1; using 192.168.0.103 instead (on interface eno1)\n",
      "22/12/08 21:17:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 21:17:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[11]\") \\\n",
    "        .config(\"spark.executor.memory\", \"16g\") \\\n",
    "        .config(\"spark.driver.memory\", \"16g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "        .config(\"spark.memory.offHeap.size\",\"16g\") \\\n",
    "        .appName(\"Airline\") \\\n",
    "        .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Pairs of Airport and Weather Stations ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read flights, airports, and weather stations data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/08 21:17:12 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    }
   ],
   "source": [
    "# All flights dataset\n",
    "flights_df = spark.read.option(\"header\",True).csv('../data/flights/')\n",
    "# GHCND stations information\n",
    "stations_df = spark.read.option(\"header\",False).csv('../data/ghcnd-stations_clean.csv')\n",
    "# United States airport information\n",
    "airports_df = spark.read.option(\"header\",True).csv('../data/us-airports_clean.csv')\n",
    "\n",
    "# Create tables for these dataframe in order to query\n",
    "flights_df.createOrReplaceTempView(\"Flights\")\n",
    "stations_df.createOrReplaceTempView(\"Stations\")\n",
    "airports_df.createOrReplaceTempView(\"Airports\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(FL_DATE='2013-01-01', OP_CARRIER='VX', OP_CARRIER_FL_NUM='108', ORIGIN='LAX', DEST='IAD', CRS_DEP_TIME='700', DEP_TIME='700.0', DEP_DELAY='0.0', TAXI_OUT='8.0', WHEELS_OFF='708.0', WHEELS_ON='1411.0', TAXI_IN='7.0', CRS_ARR_TIME='1445', ARR_TIME='1418.0', ARR_DELAY='-27.0', CANCELLED='0.0', CANCELLATION_CODE=None, DIVERTED='0.0', CRS_ELAPSED_TIME='285.0', ACTUAL_ELAPSED_TIME='258.0', AIR_TIME='243.0', DISTANCE='2288.0', CARRIER_DELAY=None, WEATHER_DELAY=None, NAS_DELAY=None, SECURITY_DELAY=None, LATE_AIRCRAFT_DELAY=None, Unnamed: 27=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flights_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=======================================================>(57 + 1) / 58]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+-------+\n",
      "|dep_max|dep_min|arr_max|arr_min|\n",
      "+-------+-------+-------+-------+\n",
      "| 2755.0| -251.0| 2692.0| -411.0|\n",
      "+-------+-------+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "flights_df.select(  \n",
    "                    max(col(\"DEP_DELAY\").cast('double')).alias('dep_max'), \\\n",
    "                    min(col(\"DEP_DELAY\").cast('double')).alias('dep_min'), \\\n",
    "                    max(col(\"ARR_DELAY\").cast('double')).alias('arr_max'), \\\n",
    "                    min(col(\"ARR_DELAY\").cast('double')).alias('arr_min') \\\n",
    "                ).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All airport codes in dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:=======================================================> (56 + 2) / 58]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ORIGIN|\n",
      "+------+\n",
      "|   BGM|\n",
      "|   INL|\n",
      "|   PSE|\n",
      "|   DLG|\n",
      "|   MSY|\n",
      "|   PPG|\n",
      "|   GEG|\n",
      "|   DRT|\n",
      "|   SNA|\n",
      "|   BUR|\n",
      "|   GRB|\n",
      "|   GTF|\n",
      "|   IFP|\n",
      "|   IDA|\n",
      "|   GRR|\n",
      "|   LWB|\n",
      "|   JLN|\n",
      "|   PVU|\n",
      "|   EUG|\n",
      "|   PSG|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "apCode_df = spark.sql(\"SELECT DISTINCT ORIGIN FROM Flights \\\n",
    "                        UNION \\\n",
    "                        SELECT DISTINCT DEST FROM Flights\")\n",
    "apCode_df.show()\n",
    "apCode_df.createOrReplaceTempView(\"Airport_Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All airline carriers in dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================================> (56 + 2) / 58]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|OP_CARRIER|\n",
      "+----------+\n",
      "|        UA|\n",
      "|        AA|\n",
      "|        EV|\n",
      "|        B6|\n",
      "|        DL|\n",
      "|        OO|\n",
      "|        F9|\n",
      "|        YV|\n",
      "|        US|\n",
      "|        MQ|\n",
      "|        HA|\n",
      "|        AS|\n",
      "|        FL|\n",
      "|        VX|\n",
      "|        WN|\n",
      "|        9E|\n",
      "|        NK|\n",
      "|        XE|\n",
      "|        CO|\n",
      "|        NW|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT DISTINCT OP_CARRIER FROM Flights\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Append full airport information with airport codes appeared in airline delay and cancellation dataset**<br/>\n",
    "columns: (ORIGIN, STATE, CITY, AP LOCAL_CODE, AP IATA_CODE, AP IDENT_CODE, AP NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "path file:/home/ax/Develop/CS543/Project1/output/airports_full already exists.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 14\u001b[0m\n\u001b[1;32m      1\u001b[0m airport_full_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39msql(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m        SELECT c.ORIGIN, \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m            p.local_region AS STATE, \u001b[39m\u001b[39m\\\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m        (c.ORIGIN = p.local_code OR c.ORIGIN = p.iata_code)\u001b[39m\u001b[39m\"\u001b[39m \\\n\u001b[1;32m     12\u001b[0m     )\n\u001b[0;32m---> 14\u001b[0m airport_full_df\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49moption(\u001b[39m\"\u001b[39;49m\u001b[39mheader\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39mTrue\u001b[39;49;00m)\u001b[39m.\u001b[39;49mcsv(\u001b[39m'\u001b[39;49m\u001b[39m../output/airports_full\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     15\u001b[0m airport_full_df\u001b[39m.\u001b[39mcreateOrReplaceTempView(\u001b[39m\"\u001b[39m\u001b[39mAirport_Full\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Develop/CS543/Project1/cs543_proj_1/lib/python3.10/site-packages/pyspark/sql/readwriter.py:1240\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[1;32m   1222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[1;32m   1223\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[1;32m   1224\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     lineSep\u001b[39m=\u001b[39mlineSep,\n\u001b[1;32m   1239\u001b[0m )\n\u001b[0;32m-> 1240\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mcsv(path)\n",
      "File \u001b[0;32m~/Develop/CS543/Project1/cs543_proj_1/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[39m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/Develop/CS543/Project1/cs543_proj_1/lib/python3.10/site-packages/pyspark/sql/utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    192\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[1;32m    193\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    194\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    195\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[1;32m    197\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: path file:/home/ax/Develop/CS543/Project1/output/airports_full already exists."
     ]
    }
   ],
   "source": [
    "airport_full_df = spark.sql(\" \\\n",
    "        SELECT c.ORIGIN, \\\n",
    "            p.local_region AS STATE, \\\n",
    "            p.municipality AS CITY, \\\n",
    "            p.local_code AS LOCAL, \\\n",
    "            p.iata_code AS IATA, \\\n",
    "            p.ident AS IDENT, \\\n",
    "            p.name AS NAME \\\n",
    "        FROM Airport_Code AS c \\\n",
    "        LEFT JOIN Airports AS p ON \\\n",
    "        (c.ORIGIN = p.local_code OR c.ORIGIN = p.iata_code)\" \\\n",
    "    )\n",
    "    \n",
    "# airport_full_df.write.option(\"header\",True).csv('../output/airports_full')\n",
    "airport_full_df.createOrReplaceTempView(\"Airport_Full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending Features to Airlines Dataset ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load preprocessed airport-station dataset and GHCND weather data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_station_df = spark.read.option(\"header\",True).csv('../data/airports_stations.csv')\n",
    "ghcnd_all = spark.read.option(\"header\",True).csv('../data/filtered_weather.csv').drop('M_FLAG','Q_FLAG','S_FLAG','OBS_TIME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport and Stations header:\n",
      "Row(ORIGIN='BGM', STATE='NY', CITY='Binghamton', LOCAL='BGM', IATA='BGM', IDENT='KBGM', NAME='Greater Binghamton/Edwin A Link field', STATION='USW00014738')\n",
      "Stations daily data header:\n",
      "Row(STATION_ID='AQW00061705', AIRPORT_CODE='PPG', DATE='20090101', ELEMENT='TMAX', DATA_VALUE='322')\n"
     ]
    }
   ],
   "source": [
    "print('Airport and Stations header:')\n",
    "print(airport_station_df.head())\n",
    "print('Stations daily data header:')\n",
    "print(ghcnd_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign each existing airport codes into weather data value and filter out weather data that not relate to our dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_airport_station = airport_station_df.select(col('ORIGIN'),col('STATION'))\n",
    "ghcnd_all = ghcnd_all.join(selected_airport_station, \\\n",
    "                                        ghcnd_all.STATION_ID == selected_airport_station.STATION, \\\n",
    "                                        \"LeftOuter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(STATION_ID='AQW00061705', AIRPORT_CODE='PPG', DATE='20090101', ELEMENT='TMAX', DATA_VALUE='322', ORIGIN='PPG', STATION='AQW00061705')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghcnd_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ouput filtered weather dataset** <br />\n",
    "Keep not null rows and rename them based on GHCND documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcnd_all.filter(ghcnd_all.ORIGIN.isNotNull()) \\\n",
    "        .select(col('_c0').alias(\"STATION_ID\"), \\\n",
    "                col('ORIGIN').alias(\"AIRPORT_CODE\"), \\\n",
    "                col('_c1').alias(\"DATE\"), \\\n",
    "                col('_c2').alias(\"ELEMENT\"), \\\n",
    "                col('_c3').alias(\"DATA_VALUE\")) \\\n",
    "        .write.option(\"header\",True).csv(\"../output/filtered_ghcnd\")\n",
    "\n",
    "# ghcnd_all.filter(ghcnd_all.ORIGIN.isNotNull()) \\\n",
    "#         .select(col('_c0').alias(\"STATION_ID\"), \\\n",
    "#                 col('ORIGIN').alias(\"AIRPORT_CODE\"), \\\n",
    "#                 col('_c1').alias(\"DATE\"), \\\n",
    "#                 col('_c2').alias(\"ELEMENT\"), \\\n",
    "#                 col('_c3').alias(\"DATA_VALUE\"), \\\n",
    "#                 col('_c4').alias(\"M_FLAG\"), \\\n",
    "#                 col('_c5').alias(\"Q_FLAG\"), \\\n",
    "#                 col('_c6').alias(\"S_FLAG\"), \\\n",
    "#                 col('_c7').alias(\"OBS_TIME\")) \\\n",
    "#         .write.option(\"header\",True).csv(\"./output/filtered_weatherData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Four extreme points in mainland of the United States***<br />\n",
    "Sumas, Washington 49°00′08.6″N 122°15′40″W – northernmost incorporated place in the 48 contiguous states<br />\n",
    "Key West, Florida 24°32′38.724″N 81°48′17.658″W – southernmost incorporated place in the contiguous 48 states<br />\n",
    "West Quoddy Head, Maine 44°48′55.4″N 66°56′59.2″W – easternmost point on the U.S. mainland<br />\n",
    "Port Orford, Oregon 42.754065°N 124.512605°W – westernmost incorporated place in the 48 contiguous states<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top_left corner has both greater latitude and greater longitude\n",
    "# Northeast,Southeast,Mid-Northeast,Mid-Southeast,Mid-Northwest,Mid-Southwest,Northwest,Southwest\n",
    "# [Top_left(lat,lon), Bottom_right(lat,lon)]\n",
    "SE = [(37,80),(24,65)]\n",
    "NE = [(50,80),(37,65)]\n",
    "MSE = [(37,95),(24,80)]\n",
    "MNE = [(50,95),(37,80)]\n",
    "MSW = [(37,110),(24,95)]\n",
    "MNW = [(50,110),(37,95)]\n",
    "SW = [(37,125),(24,110)]\n",
    "NW = [(50,125),(37,110)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add geological coordinates for each station**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stations_info = spark.read.option(\"header\",False).csv('../data/ghcnd-stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load geological info for each airport and station**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = airport_station_df \\\n",
    "    .join(full_stations_info,airport_station_df.STATION == full_stations_info._c0,'leftouter') \\\n",
    "    .select(col('ORIGIN').alias(\"AIRPORT_CODE\"), \\\n",
    "            col('STATE'), \\\n",
    "            col('CITY'), \\\n",
    "            col('NAME'), \\\n",
    "            col('STATION'), \\\n",
    "            col('_c1').alias('LAITITUDE').cast('Double'), \\\n",
    "            col('_c2').alias('LONGITUDE').cast('Double') \\\n",
    "        ).withColumn('LONGITUDE', col('LONGITUDE')*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check max and min of LAITITUDE and LONGITUDE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo.select( \\\n",
    "            max(col('LAITITUDE')), \\\n",
    "            min(col('LAITITUDE')), \\\n",
    "            max(col('LONGITUDE')), \\\n",
    "            min(col('LONGITUDE')), \\\n",
    "        ).write.option('header',True).csv('../output/geo_max_min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assign area for each airport/station__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = geo.withColumn(\"AREA\",when((geo.LAITITUDE < SE[0][0]) & (geo.LONGITUDE < SE[0][1]),\"SE\") \\\n",
    "                                                .when((geo.LAITITUDE < NE[0][0]) & (geo.LONGITUDE < NE[0][1]),\"NE\") \\\n",
    "                                                .when((geo.LAITITUDE < MSE[0][0]) & (geo.LONGITUDE < MSE[0][1]),\"MSE\") \\\n",
    "                                                .when((geo.LAITITUDE < MNE[0][0]) & (geo.LONGITUDE < MNE[0][1]),\"MNE\") \\\n",
    "                                                .when((geo.LAITITUDE < MSW[0][0]) & (geo.LONGITUDE < MSW[0][1]),\"MSW\") \\\n",
    "                                                .when((geo.LAITITUDE < MNW[0][0]) & (geo.LONGITUDE < MNW[0][1]),\"MNW\") \\\n",
    "                                                .when((geo.LAITITUDE < SW[0][0]) & (geo.LONGITUDE < SW[0][1]),\"SW\") \\\n",
    "                                                .when((geo.LAITITUDE < NW[0][0]) & (geo.LONGITUDE < NW[0][1]),\"NW\") \\\n",
    "                                                .when((geo.LAITITUDE > NW[0][0]) & (geo.LONGITUDE > NW[0][1]),\"NW\") \\\n",
    "                                                .when((geo.LAITITUDE < SW[0][0]) & (geo.LONGITUDE > SW[0][1]),\"SW\") \\\n",
    "                                                .when((geo.LAITITUDE < SE[0][0]) & (geo.LONGITUDE < SE[0][1]),\"SE\") \\\n",
    "                                                .when((geo.LAITITUDE > NE[0][0]) & (geo.LONGITUDE < NE[0][1]),\"NE\") \\\n",
    "                                                .otherwise(geo.STATION)) \\\n",
    "                            .drop('LAITITUDE','LONGITUDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas.write.option('header',True).csv('../output/areas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data appending for flights in 2018 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load preprocessed airport-station dataset and GHCND weather data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_stations =  spark.read.option(\"header\",True).csv('../data/airports_stations.csv')\n",
    "# ghcnd2018 = spark.read.option(\"header\",False).csv('./data/filtered_weather.csv')\n",
    "ghcnd2018 = ghcnd_all\n",
    "fl2018 = spark.read.option(\"header\",True).csv('../data/flights/2018.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(STATION_ID='AQW00061705', AIRPORT_CODE='PPG', DATE='20090101', ELEMENT='TMAX', DATA_VALUE='322', ORIGIN='PPG', STATION='AQW00061705')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghcnd2018.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign each existing airport codes into weather data value and filter out weather data that not relate to our dataset.** <br/>\n",
    "**Keep not null rows and rename them based on GHCND documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute '_c0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ghcnd2018 \u001b[39m=\u001b[39m ghcnd2018\u001b[39m.\u001b[39mjoin(airports_stations, \\\n\u001b[0;32m----> 2\u001b[0m                             ghcnd2018\u001b[39m.\u001b[39;49m_c0 \u001b[39m==\u001b[39m airports_stations\u001b[39m.\u001b[39mSTATION, \\\n\u001b[1;32m      3\u001b[0m                             \u001b[39m\"\u001b[39m\u001b[39mLeftOuter\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m ghcnd2018 \u001b[39m=\u001b[39m ghcnd2018\u001b[39m.\u001b[39mfilter(ghcnd2018\u001b[39m.\u001b[39mORIGIN\u001b[39m.\u001b[39misNotNull()) \\\n\u001b[1;32m      5\u001b[0m                                 \u001b[39m.\u001b[39mselect(col(\u001b[39m'\u001b[39m\u001b[39m_c0\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mSTATION_ID\u001b[39m\u001b[39m\"\u001b[39m), \\\n\u001b[1;32m      6\u001b[0m                                         col(\u001b[39m'\u001b[39m\u001b[39mORIGIN\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mAIRPORT_CODE\u001b[39m\u001b[39m\"\u001b[39m), \\\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m                                         col(\u001b[39m'\u001b[39m\u001b[39m_c6\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mS_FLAG\u001b[39m\u001b[39m\"\u001b[39m), \\\n\u001b[1;32m     13\u001b[0m                                         col(\u001b[39m'\u001b[39m\u001b[39m_c7\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39malias(\u001b[39m\"\u001b[39m\u001b[39mOBS_TIME\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/Develop/CS543/Project1/cs543_proj_1/lib/python3.10/site-packages/pyspark/sql/dataframe.py:1988\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1978\u001b[0m \u001b[39m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   1979\u001b[0m \n\u001b[1;32m   1980\u001b[0m \u001b[39m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1985\u001b[0m \u001b[39m[Row(age=2), Row(age=5)]\u001b[39;00m\n\u001b[1;32m   1986\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1987\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns:\n\u001b[0;32m-> 1988\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[1;32m   1989\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name)\n\u001b[1;32m   1990\u001b[0m     )\n\u001b[1;32m   1991\u001b[0m jc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jdf\u001b[39m.\u001b[39mapply(name)\n\u001b[1;32m   1992\u001b[0m \u001b[39mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_c0'"
     ]
    }
   ],
   "source": [
    "ghcnd2018 = ghcnd2018.join(airports_stations, \\\n",
    "                            ghcnd2018._c0 == airports_stations.STATION, \\\n",
    "                            \"LeftOuter\")\n",
    "ghcnd2018 = ghcnd2018.filter(ghcnd2018.ORIGIN.isNotNull()) \\\n",
    "                                .select(col('_c0').alias(\"STATION_ID\"), \\\n",
    "                                        col('ORIGIN').alias(\"AIRPORT_CODE\"), \\\n",
    "                                        col('_c1').alias(\"DATE\"), \\\n",
    "                                        col('_c2').alias(\"ELEMENT\"), \\\n",
    "                                        col('_c3').alias(\"DATA_VALUE\"), \\\n",
    "                                        col('_c4').alias(\"M_FLAG\"), \\\n",
    "                                        col('_c5').alias(\"Q_FLAG\"), \\\n",
    "                                        col('_c6').alias(\"S_FLAG\"), \\\n",
    "                                        col('_c7').alias(\"OBS_TIME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter wanted features and join area category for ORIGIN and DEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fl2018.select(['FL_DATE','OP_CARRIER','OP_CARRIER_FL_NUM','ORIGIN','DEST','CRS_DEP_TIME','DEP_DELAY','CRS_ARR_TIME','ARR_DELAY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(areas, fl2018.ORIGIN == areas.AIRPORT_CODE,'LeftOuter') \\\n",
    "                .drop('AIRPORT_CODE','STATE','CITY','NAME') \\\n",
    "                .withColumnRenamed('AREA', 'ORIG_AREA').drop('ORIGIN') \\\n",
    "                .withColumnRenamed('STATION', 'ORIG_STATION') \\\n",
    "        .join(areas, fl2018.DEST == areas.AIRPORT_CODE,'LeftOuter') \\\n",
    "                .drop('AIRPORT_CODE','STATE','CITY','NAME') \\\n",
    "                .withColumnRenamed('AREA', 'DEST_AREA').drop('DEST') \\\n",
    "                .withColumnRenamed('STATION', 'DEST_STATION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Append weather info according to ORIGIN stations and DEST stations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(ghcnd2018,(df.ORIG_STATION == ghcnd2018.STATION_ID) & (ghcnd2018.DATE == date_format(df.FL_DATE,\"yyyyMMdd\")),'Inner') \\\n",
    "                .drop('STATION','AIRPORT_CODE','DATE','STATION_ID') \\\n",
    "                .withColumnRenamed('ELEMENT', 'ORIG_WEATHER') \\\n",
    "                .withColumnRenamed('DATA_VALUE', 'ORIG_WEATHER_DATA') \\\n",
    "        .join(ghcnd2018, (df.DEST_STATION == ghcnd2018.STATION_ID) & (ghcnd2018.DATE == date_format(df.FL_DATE,\"yyyyMMdd\")),'Inner') \\\n",
    "                .drop('STATION','AIRPORT_CODE','DATE','STATION_ID') \\\n",
    "                .withColumnRenamed('ELEMENT', 'DEST_WEATHER') \\\n",
    "                .withColumnRenamed('DATA_VALUE', 'DEST_WEATHER_DATA') \\\n",
    "        .filter(((col('ORIG_WEATHER_DATA').cast('Double') != 0) & (col('ORIG_WEATHER').rlike('(PRCP|SNOW|SNWD|^WD+|^WS+)'))) | \\\n",
    "                ((col('DEST_WEATHER_DATA').cast('Double') != 0) & (col('DEST_WEATHER').rlike('(PRCP|SNOW|SNWD|^WD+|^WS+)')))) \\\n",
    "        .drop('ORIG_WEATHER_DATA','DEST_WEATHER_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('ORIG_WEATHER', when(~df.ORIG_WEATHER.rlike('(PRCP|SNOW|SNWD|^WD+|^WS+)'),\"SUNNY\") \\\n",
    "                                    .otherwise(df.ORIG_WEATHER)) \\\n",
    "        .withColumn('DEST_WEATHER', when(~df.DEST_WEATHER.rlike('(PRCP|SNOW|SNWD|^WD+|^WS+)'),\"SUNNY\") \\\n",
    "                                    .otherwise(df.DEST_WEATHER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy('FL_DATE', \\\n",
    "                'OP_CARRIER', \\\n",
    "                'OP_CARRIER_FL_NUM', \\\n",
    "                'CRS_DEP_TIME', \\\n",
    "                'DEP_DELAY', \\\n",
    "                'CRS_ARR_TIME', \\\n",
    "                'ARR_DELAY', \\\n",
    "                'ORIG_STATION', \\\n",
    "                'ORIG_AREA', \\\n",
    "                'DEST_STATION', \\\n",
    "                'DEST_AREA') \\\n",
    "        .agg(collect_list('ORIG_WEATHER').alias('ORIG_WEATHERS'), collect_list('DEST_WEATHER').alias('DEST_WEATHERS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chose Represented Weather**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('ORIG_WEATHER', when((array_contains(df.ORIG_WEATHERS,'SNWD') | array_contains(df.ORIG_WEATHERS,'SNOW') | array_contains(df.ORIG_WEATHERS,'WT18')),'Snow') \\\n",
    "                                    .when((array_contains(df.ORIG_WEATHERS,'PRCP') | array_contains(df.ORIG_WEATHERS,'WT16')),'Rain') \\\n",
    "                                    .when((array_contains(df.ORIG_WEATHERS,'WDF2') | array_contains(df.ORIG_WEATHERS,'WDF5') | array_contains(df.ORIG_WEATHERS,'WT11')),'Wind') \\\n",
    "                                    .otherwise('Sunny')) \\\n",
    "        .withColumn('DEST_WEATHER', when((array_contains(df.DEST_WEATHERS,'SNWD') | array_contains(df.DEST_WEATHERS,'SNOW') | array_contains(df.DEST_WEATHERS,'WT18')),'Snow') \\\n",
    "                                    .when((array_contains(df.DEST_WEATHERS,'PRCP') | array_contains(df.DEST_WEATHERS,'WT16')),'Rain') \\\n",
    "                                    .when((array_contains(df.DEST_WEATHERS,'WDF2') | array_contains(df.DEST_WEATHERS,'WDF5') | array_contains(df.DEST_WEATHERS,'WT11')),'Wind') \\\n",
    "                                    .otherwise('Sunny'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('ORIG_WEATHERS','DEST_WEATHERS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other features' category:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stations_info = spark.read.option(\"header\",False).csv('../data/ghcnd-stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply busy/not busy categories for ORIG and DEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('ORIG_SCHEDULE', \\\n",
    "                    when((df.CRS_DEP_TIME.cast('Integer') >= 800) & (df.CRS_DEP_TIME < 1800),\"Most Busy\") \\\n",
    "                    .when((df.CRS_DEP_TIME.cast('Integer') >= 600) & (df.CRS_DEP_TIME < 800),\"Least Busy\") \\\n",
    "                    .otherwise(\"Busy\")) \\\n",
    "        .withColumn('DEST_SCHEDULE', \\\n",
    "                    when((df.CRS_ARR_TIME.cast('Integer') >= 800) & (df.CRS_ARR_TIME < 1800),\"Most Busy\") \\\n",
    "                    .when((df.CRS_ARR_TIME.cast('Integer') >= 600) & (df.CRS_ARR_TIME < 800),\"Least Busy\") \\\n",
    "                    .otherwise(\"Busy\")) \\\n",
    "        .drop('CRS_DEP_TIME','CRS_ARR_TIME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply carrier type categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers = spark.read.option(\"header\",True).csv('../data/carriers_type.csv')\n",
    "\n",
    "df = df.join(carriers, df.OP_CARRIER == carriers.OP_CARRIER, 'LeftOuter') \\\n",
    "        .drop('OP_CARRIER') \\\n",
    "        .withColumnRenamed('CATEGORY','CARRIER_TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply delay categories for DEPART and ARRIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('DEP_DELAY_CAT', \n",
    "                when((df.DEP_DELAY.cast('Double') < -60),\"60+min early\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') >= -60) & (df.DEP_DELAY < -30),\"30~60min early\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') >= -30) & (df.DEP_DELAY < -15),\"15~30min early\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') >= -15) & (df.DEP_DELAY < -5),\"5~15min early\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') >= -5) & (df.DEP_DELAY <= 5),\"On Time\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') > 5) & (df.DEP_DELAY <= 15),\"5~15min late\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') > 15) & (df.DEP_DELAY <= 30),\"15~30min late\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') > 30) & (df.DEP_DELAY <= 60),\"30~60min late\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') > -60),\"60+min late\") \\\n",
    "                .otherwise(\"On Time\")) \\\n",
    "    .withColumn('ARR_DELAY_CAT', \n",
    "                when((df.ARR_DELAY.cast('Double') < -60),\"60+min early\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') >= -60) & (df.ARR_DELAY < -30),\"30~60min early\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') >= -30) & (df.ARR_DELAY < -15),\"15~30min early\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') >= -15) & (df.ARR_DELAY < -5),\"5~15min early\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') >= -5) & (df.ARR_DELAY <= 5),\"On Time\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') > 5) & (df.ARR_DELAY <= 15),\"5~15min late\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') > 15) & (df.ARR_DELAY <= 30),\"15~30min late\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') > 30) & (df.ARR_DELAY <= 60),\"30~60min late\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') > -60),\"60+min late\") \\\n",
    "                .otherwise(\"On Time\")) \\\n",
    "    .drop('DEP_DELAY','ARR_DELAY')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop useless data and Output the csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('FL_DATE','OP_CARRIER_FL_NUM','ORIG_STATION','DEST_STATION','ORIG_WEATHERS','DEST_WEATHERS')\n",
    "\n",
    "df.coalesce(1).write.option('header',True).csv('../output/2018')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cs543_proj_1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cde981a51954932d95cd0d2db88350bc6a3d7387d257ee815a180cd40a004a1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
