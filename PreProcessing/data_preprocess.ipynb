{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create or get a PySpark instance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 08:29:40 WARN Utils: Your hostname, LinuxGUI resolves to a loopback address: 127.0.1.1; using 192.168.0.103 instead (on interface eno1)\n",
      "22/11/30 08:29:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/30 08:29:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .master(\"local[11]\") \\\n",
    "        .config(\"spark.executor.memory\", \"16g\") \\\n",
    "        .config(\"spark.driver.memory\", \"16g\") \\\n",
    "        .config(\"spark.memory.offHeap.enabled\",True) \\\n",
    "        .config(\"spark.memory.offHeap.size\",\"16g\") \\\n",
    "        .appName(\"Airline\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "\n",
    "        # .config(\"spark.sql.shuffle.partitions\",60) \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sparkContext.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Pairs of Airport and Weather Stations ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Read flights, airports, and weather stations data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All flights dataset\n",
    "flights_df = spark.read.option(\"header\",True).csv('./data/flights/')\n",
    "# GHCND stations information\n",
    "stations_df = spark.read.option(\"header\",False).csv('./data/ghcnd-stations_clean.csv')\n",
    "# United States airport information\n",
    "airports_df = spark.read.option(\"header\",True).csv('./data/us-airports_clean.csv')\n",
    "\n",
    "# Create tables for these dataframe in order to query\n",
    "flights_df.createOrReplaceTempView(\"Flights\")\n",
    "stations_df.createOrReplaceTempView(\"Stations\")\n",
    "airports_df.createOrReplaceTempView(\"Airports\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Trying: Concat all flights files (10) into 1 csv file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flights_df.coalesce(1).write.option(\"header\", True).csv('./output/concat_flights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All airport codes in dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apCode_df = spark.sql(\"SELECT DISTINCT ORIGIN FROM Flights \\\n",
    "                        UNION \\\n",
    "                        SELECT DISTINCT DEST FROM Flights\")\n",
    "apCode_df.createOrReplaceTempView(\"Airport_Code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All airline carriers in dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"SELECT DISTINCT OP_CARRIER FROM Flights\").write.option(\"header\",True).csv('./output/all_carriers')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Append full airport information with airport codes appeared in airline delay and cancellation dataset**<br/>\n",
    "columns: (ORIGIN, STATE, CITY, AP LOCAL_CODE, AP IATA_CODE, AP IDENT_CODE, AP NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_full_df = spark.sql(\" \\\n",
    "        SELECT c.ORIGIN, \\\n",
    "            p.local_region AS STATE, \\\n",
    "            p.municipality AS CITY, \\\n",
    "            p.local_code AS LOCAL, \\\n",
    "            p.iata_code AS IATA, \\\n",
    "            p.ident AS IDENT, \\\n",
    "            p.name AS NAME \\\n",
    "        FROM Airport_Code AS c \\\n",
    "        LEFT JOIN Airports AS p ON \\\n",
    "        (c.ORIGIN = p.local_code OR c.ORIGIN = p.iata_code)\" \\\n",
    "    )\n",
    "    \n",
    "airport_full_df.write.option(\"header\",True).csv('./output/airports_full')\n",
    "airport_full_df.createOrReplaceTempView(\"Airport_Full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appending Features to Airlines Dataset ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Four extreme points in mainland of the United States***<br />\n",
    "Sumas, Washington 49°00′08.6″N 122°15′40″W – northernmost incorporated place in the 48 contiguous states<br />\n",
    "Key West, Florida 24°32′38.724″N 81°48′17.658″W – southernmost incorporated place in the contiguous 48 states<br />\n",
    "West Quoddy Head, Maine 44°48′55.4″N 66°56′59.2″W – easternmost point on the U.S. mainland<br />\n",
    "Port Orford, Oregon 42.754065°N 124.512605°W – westernmost incorporated place in the 48 contiguous states<br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2013\n",
    "# top_left corner has both greater latitude and greater longitude\n",
    "# Northeast,Southeast,Mid-Northeast,Mid-Southeast,Mid-Northwest,Mid-Southwest,Northwest,Southwest\n",
    "# [Top_left(lat,lon), Bottom_right(lat,lon)]\n",
    "SE = [(37,80),(24,65)]\n",
    "NE = [(50,80),(37,65)]\n",
    "MSE = [(37,95),(24,80)]\n",
    "MNE = [(50,95),(37,80)]\n",
    "MSW = [(37,110),(24,95)]\n",
    "MNW = [(50,110),(37,95)]\n",
    "SW = [(37,125),(24,110)]\n",
    "NW = [(50,125),(37,110)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load preprocessed airport-station dataset and GHCND weather data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "airport_station_df = spark.read.option(\"header\",True).csv('./data/airports_stations.csv')\n",
    "ghcnd_all = spark.read.option(\"header\",True).csv('./data/filtered_weather.csv').drop('M_FLAG','Q_FLAG','S_FLAG','OBS_TIME')\n",
    "# ghcnd_all = spark.read.option(\"header\",False).csv('./data/ghcnd_by_year/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airport and Stations header:\n",
      "Row(ORIGIN='BGM', STATE='NY', CITY='Binghamton', LOCAL='BGM', IATA='BGM', IDENT='KBGM', NAME='Greater Binghamton/Edwin A Link field', STATION='USW00014738')\n",
      "Stations daily data header:\n",
      "Row(STATION_ID='AQW00061705', AIRPORT_CODE='PPG', DATE='20090101', ELEMENT='TMAX', DATA_VALUE='322')\n"
     ]
    }
   ],
   "source": [
    "print('Airport and Stations header:')\n",
    "print(airport_station_df.head())\n",
    "print('Stations daily data header:')\n",
    "print(ghcnd_all.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign each existing airport codes into weather data value and filter out weather data that not relate to our dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_airport_station = airport_station_df.select(col('ORIGIN'),col('STATION'))\n",
    "ghcnd_all = ghcnd_all.join(selected_airport_station, \\\n",
    "                                        ghcnd_all.STATION_ID == selected_airport_station.STATION, \\\n",
    "                                        \"LeftOuter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(STATION_ID='AQW00061705', AIRPORT_CODE='PPG', DATE='20090101', ELEMENT='TMAX', DATA_VALUE='322', ORIGIN='PPG', STATION='AQW00061705')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghcnd_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ouput filtered weather dataset** <br />\n",
    "Keep not null rows and rename them based on GHCND documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcnd_all.filter(ghcnd_all.ORIGIN.isNotNull()) \\\n",
    "        .select(col('_c0').alias(\"STATION_ID\"), \\\n",
    "                col('ORIGIN').alias(\"AIRPORT_CODE\"), \\\n",
    "                col('_c1').alias(\"DATE\"), \\\n",
    "                col('_c2').alias(\"ELEMENT\"), \\\n",
    "                col('_c3').alias(\"DATA_VALUE\")) \\\n",
    "        .write.option(\"header\",True).csv(\"./output/filtered_ghcnd\")\n",
    "\n",
    "# ghcnd_all.filter(ghcnd_all.ORIGIN.isNotNull()) \\\n",
    "#         .select(col('_c0').alias(\"STATION_ID\"), \\\n",
    "#                 col('ORIGIN').alias(\"AIRPORT_CODE\"), \\\n",
    "#                 col('_c1').alias(\"DATE\"), \\\n",
    "#                 col('_c2').alias(\"ELEMENT\"), \\\n",
    "#                 col('_c3').alias(\"DATA_VALUE\"), \\\n",
    "#                 col('_c4').alias(\"M_FLAG\"), \\\n",
    "#                 col('_c5').alias(\"Q_FLAG\"), \\\n",
    "#                 col('_c6').alias(\"S_FLAG\"), \\\n",
    "#                 col('_c7').alias(\"OBS_TIME\")) \\\n",
    "#         .write.option(\"header\",True).csv(\"./output/filtered_weatherData\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add geological coordinates for each station**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stations_info = spark.read.option(\"header\",False).csv('./data/ghcnd-stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load geological info for each airport and station**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo = airport_station_df \\\n",
    "    .join(full_stations_info,airport_station_df.STATION == full_stations_info._c0,'leftouter') \\\n",
    "    .select(col('ORIGIN').alias(\"AIRPORT_CODE\"), \\\n",
    "            col('STATE'), \\\n",
    "            col('CITY'), \\\n",
    "            col('NAME'), \\\n",
    "            col('STATION'), \\\n",
    "            col('_c1').alias('LAITITUDE').cast('Double'), \\\n",
    "            col('_c2').alias('LONGITUDE').cast('Double') \\\n",
    "        ).withColumn('LONGITUDE', col('LONGITUDE')*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check max and min of LAITITUDE and LONGITUDE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo.select( \\\n",
    "            max(col('LAITITUDE')), \\\n",
    "            min(col('LAITITUDE')), \\\n",
    "            max(col('LONGITUDE')), \\\n",
    "            min(col('LONGITUDE')), \\\n",
    "        ).write.option('header',True).csv('./output/geo_max_min')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Assign area for each airport/station__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = geo.withColumn(\"AREA\",when((geo.LAITITUDE < SE[0][0]) & (geo.LONGITUDE < SE[0][1]),\"SE\") \\\n",
    "                                                .when((geo.LAITITUDE < NE[0][0]) & (geo.LONGITUDE < NE[0][1]),\"NE\") \\\n",
    "                                                .when((geo.LAITITUDE < MSE[0][0]) & (geo.LONGITUDE < MSE[0][1]),\"MSE\") \\\n",
    "                                                .when((geo.LAITITUDE < MNE[0][0]) & (geo.LONGITUDE < MNE[0][1]),\"MNE\") \\\n",
    "                                                .when((geo.LAITITUDE < MSW[0][0]) & (geo.LONGITUDE < MSW[0][1]),\"MSW\") \\\n",
    "                                                .when((geo.LAITITUDE < MNW[0][0]) & (geo.LONGITUDE < MNW[0][1]),\"MNW\") \\\n",
    "                                                .when((geo.LAITITUDE < SW[0][0]) & (geo.LONGITUDE < SW[0][1]),\"SW\") \\\n",
    "                                                .when((geo.LAITITUDE < NW[0][0]) & (geo.LONGITUDE < NW[0][1]),\"NW\") \\\n",
    "                                                .when((geo.LAITITUDE > NW[0][0]) & (geo.LONGITUDE > NW[0][1]),\"NW\") \\\n",
    "                                                .when((geo.LAITITUDE < SW[0][0]) & (geo.LONGITUDE > SW[0][1]),\"SW\") \\\n",
    "                                                .when((geo.LAITITUDE < SE[0][0]) & (geo.LONGITUDE < SE[0][1]),\"SE\") \\\n",
    "                                                .when((geo.LAITITUDE > NE[0][0]) & (geo.LONGITUDE < NE[0][1]),\"NE\") \\\n",
    "                                                .otherwise(geo.STATION)) \\\n",
    "                            .drop('LAITITUDE','LONGITUDE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas.write.option('header',True).csv('./output/areas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data appending for flights in 2018 ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load preprocessed airport-station dataset and GHCND weather data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "airports_stations =  spark.read.option(\"header\",True).csv('./data/airports_stations.csv')\n",
    "# ghcnd2018 = spark.read.option(\"header\",False).csv('./data/filtered_weather.csv')\n",
    "ghcnd2018 = ghcnd_all\n",
    "fl2018 = spark.read.option(\"header\",True).csv('./data/flights/{}.csv'.format(YEAR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(STATION_ID='AQW00061705', AIRPORT_CODE='PPG', DATE='20090101', ELEMENT='TMAX', DATA_VALUE='322', ORIGIN='PPG', STATION='AQW00061705')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ghcnd2018.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Assign each existing airport codes into weather data value and filter out weather data that not relate to our dataset.** <br/>\n",
    "**Keep not null rows and rename them based on GHCND documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ghcnd2018 = ghcnd2018.join(airports_stations, \\\n",
    "                            ghcnd2018._c0 == airports_stations.STATION, \\\n",
    "                            \"LeftOuter\")\n",
    "ghcnd2018 = ghcnd2018.filter(ghcnd2018.ORIGIN.isNotNull()) \\\n",
    "                                .select(col('_c0').alias(\"STATION_ID\"), \\\n",
    "                                        col('ORIGIN').alias(\"AIRPORT_CODE\"), \\\n",
    "                                        col('_c1').alias(\"DATE\"), \\\n",
    "                                        col('_c2').alias(\"ELEMENT\"), \\\n",
    "                                        col('_c3').alias(\"DATA_VALUE\"), \\\n",
    "                                        col('_c4').alias(\"M_FLAG\"), \\\n",
    "                                        col('_c5').alias(\"Q_FLAG\"), \\\n",
    "                                        col('_c6').alias(\"S_FLAG\"), \\\n",
    "                                        col('_c7').alias(\"OBS_TIME\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter wanted features and join area category for ORIGIN and DEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fl2018.select(['FL_DATE','OP_CARRIER','OP_CARRIER_FL_NUM','ORIGIN','DEST','CRS_DEP_TIME','DEP_DELAY','CRS_ARR_TIME','ARR_DELAY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(areas, fl2018.ORIGIN == areas.AIRPORT_CODE,'LeftOuter') \\\n",
    "                .drop('AIRPORT_CODE','STATE','CITY','NAME') \\\n",
    "                .withColumnRenamed('AREA', 'ORIG_AREA').drop('ORIGIN') \\\n",
    "                .withColumnRenamed('STATION', 'ORIG_STATION') \\\n",
    "        .join(areas, fl2018.DEST == areas.AIRPORT_CODE,'LeftOuter') \\\n",
    "                .drop('AIRPORT_CODE','STATE','CITY','NAME') \\\n",
    "                .withColumnRenamed('AREA', 'DEST_AREA').drop('DEST') \\\n",
    "                .withColumnRenamed('STATION', 'DEST_STATION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Append weather info according to ORIGIN stations and DEST stations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.join(ghcnd2018,(df.ORIG_STATION == ghcnd2018.STATION_ID) & (ghcnd2018.DATE == date_format(df.FL_DATE,\"yyyyMMdd\")),'Inner') \\\n",
    "                .drop('STATION','AIRPORT_CODE','DATE','STATION_ID') \\\n",
    "                .withColumnRenamed('ELEMENT', 'ORIG_WEATHER') \\\n",
    "                .withColumnRenamed('DATA_VALUE', 'ORIG_WEATHER_DATA') \\\n",
    "        .join(ghcnd2018, (df.DEST_STATION == ghcnd2018.STATION_ID) & (ghcnd2018.DATE == date_format(df.FL_DATE,\"yyyyMMdd\")),'Inner') \\\n",
    "                .drop('STATION','AIRPORT_CODE','DATE','STATION_ID') \\\n",
    "                .withColumnRenamed('ELEMENT', 'DEST_WEATHER') \\\n",
    "                .withColumnRenamed('DATA_VALUE', 'DEST_WEATHER_DATA') \\\n",
    "        .filter(((col('ORIG_WEATHER_DATA').cast('Double') != 0) & (col('ORIG_WEATHER').rlike('(PRCP|SNOW|SNWD|^WD+|^WS+)'))) | \\\n",
    "                ((col('DEST_WEATHER_DATA').cast('Double') != 0) & (col('DEST_WEATHER').rlike('(PRCP|SNOW|SNWD|^WD+|^WS+)')))) \\\n",
    "        .drop('ORIG_WEATHER_DATA','DEST_WEATHER_DATA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('ORIG_WEATHER', when(~df.ORIG_WEATHER.rlike('(PRCP|SNOW|SNWD|^WD+|^WS+)'),\"SUNNY\") \\\n",
    "                                    .otherwise(df.ORIG_WEATHER)) \\\n",
    "        .withColumn('DEST_WEATHER', when(~df.DEST_WEATHER.rlike('(PRCP|SNOW|SNWD|^WD+|^WS+)'),\"SUNNY\") \\\n",
    "                                    .otherwise(df.DEST_WEATHER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.groupBy('FL_DATE', \\\n",
    "                'OP_CARRIER', \\\n",
    "                'OP_CARRIER_FL_NUM', \\\n",
    "                'CRS_DEP_TIME', \\\n",
    "                'DEP_DELAY', \\\n",
    "                'CRS_ARR_TIME', \\\n",
    "                'ARR_DELAY', \\\n",
    "                'ORIG_STATION', \\\n",
    "                'ORIG_AREA', \\\n",
    "                'DEST_STATION', \\\n",
    "                'DEST_AREA') \\\n",
    "        .agg(collect_list('ORIG_WEATHER').alias('ORIG_WEATHERS'), collect_list('DEST_WEATHER').alias('DEST_WEATHERS'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Chose Represented Weather**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('ORIG_WEATHER', when((array_contains(df.ORIG_WEATHERS,'SNWD') | array_contains(df.ORIG_WEATHERS,'SNOW') | array_contains(df.ORIG_WEATHERS,'WT18')),'Snow') \\\n",
    "                                    .when((array_contains(df.ORIG_WEATHERS,'PRCP') | array_contains(df.ORIG_WEATHERS,'WT16')),'Rain') \\\n",
    "                                    .when((array_contains(df.ORIG_WEATHERS,'WDF2') | array_contains(df.ORIG_WEATHERS,'WDF5') | array_contains(df.ORIG_WEATHERS,'WT11')),'Wind') \\\n",
    "                                    .otherwise('Sunny')) \\\n",
    "        .withColumn('DEST_WEATHER', when((array_contains(df.DEST_WEATHERS,'SNWD') | array_contains(df.DEST_WEATHERS,'SNOW') | array_contains(df.DEST_WEATHERS,'WT18')),'Snow') \\\n",
    "                                    .when((array_contains(df.DEST_WEATHERS,'PRCP') | array_contains(df.DEST_WEATHERS,'WT16')),'Rain') \\\n",
    "                                    .when((array_contains(df.DEST_WEATHERS,'WDF2') | array_contains(df.DEST_WEATHERS,'WDF5') | array_contains(df.DEST_WEATHERS,'WT11')),'Wind') \\\n",
    "                                    .otherwise('Sunny'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('ORIG_WEATHERS','DEST_WEATHERS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other features' category:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_stations_info = spark.read.option(\"header\",False).csv('./data/ghcnd-stations.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply busy/not busy categories for ORIG and DEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('ORIG_SCHEDULE', \\\n",
    "                    when((df.CRS_DEP_TIME.cast('Integer') >= 800) & (df.CRS_DEP_TIME < 1800),\"Most Busy\") \\\n",
    "                    .when((df.CRS_DEP_TIME.cast('Integer') >= 600) & (df.CRS_DEP_TIME < 800),\"Least Busy\") \\\n",
    "                    .otherwise(\"Busy\")) \\\n",
    "        .withColumn('DEST_SCHEDULE', \\\n",
    "                    when((df.CRS_ARR_TIME.cast('Integer') >= 800) & (df.CRS_ARR_TIME < 1800),\"Most Busy\") \\\n",
    "                    .when((df.CRS_ARR_TIME.cast('Integer') >= 600) & (df.CRS_ARR_TIME < 800),\"Least Busy\") \\\n",
    "                    .otherwise(\"Busy\")) \\\n",
    "        .drop('CRS_DEP_TIME','CRS_ARR_TIME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply carrier type categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "carriers = spark.read.option(\"header\",True).csv('./data/carriers_type.csv')\n",
    "\n",
    "df = df.join(carriers, df.OP_CARRIER == carriers.OP_CARRIER, 'LeftOuter') \\\n",
    "        .drop('OP_CARRIER') \\\n",
    "        .withColumnRenamed('CATEGORY','CARRIER_TYPE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Apply delay categories for DEPART and ARRIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn('DEP_DELAY_CAT', \n",
    "                when((df.DEP_DELAY.cast('Double') < -60),\"60+min early\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') >= -60) & (df.DEP_DELAY < -30),\"30~60min early\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') >= -30) & (df.DEP_DELAY < -15),\"15~30min early\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') >= -15) & (df.DEP_DELAY < -5),\"5~15min early\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') >= -5) & (df.DEP_DELAY <= 5),\"On Time\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') > 5) & (df.DEP_DELAY <= 15),\"5~15min late\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') > 15) & (df.DEP_DELAY <= 30),\"15~30min late\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') > 30) & (df.DEP_DELAY <= 60),\"30~60min late\") \\\n",
    "                .when((df.DEP_DELAY.cast('Double') > -60),\"60+min late\") \\\n",
    "                .otherwise(\"On Time\")) \\\n",
    "    .withColumn('ARR_DELAY_CAT', \n",
    "                when((df.ARR_DELAY.cast('Double') < -60),\"60+min early\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') >= -60) & (df.ARR_DELAY < -30),\"30~60min early\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') >= -30) & (df.ARR_DELAY < -15),\"15~30min early\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') >= -15) & (df.ARR_DELAY < -5),\"5~15min early\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') >= -5) & (df.ARR_DELAY <= 5),\"On Time\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') > 5) & (df.ARR_DELAY <= 15),\"5~15min late\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') > 15) & (df.ARR_DELAY <= 30),\"15~30min late\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') > 30) & (df.ARR_DELAY <= 60),\"30~60min late\") \\\n",
    "                .when((df.ARR_DELAY.cast('Double') > -60),\"60+min late\") \\\n",
    "                .otherwise(\"On Time\")) \\\n",
    "    .drop('DEP_DELAY','ARR_DELAY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Drop useless data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('FL_DATE','OP_CARRIER_FL_NUM','ORIG_STATION','DEST_STATION','ORIG_WEATHERS','DEST_WEATHERS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.coalesce(1).write.option('header',True).csv('./output/{}'.format(YEAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Combine outputs into one file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_int = spark.read.option(\"header\",True).csv('./output/all_ann/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "processed_int.coalesce(1).write.option('header',False).csv('./output/processed_ann')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cs543_proj_1': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "cde981a51954932d95cd0d2db88350bc6a3d7387d257ee815a180cd40a004a1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
